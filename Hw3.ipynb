{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CSCE 320 :: Principles of Data Science ::Texas A&M University :: Fall 2020\n",
    "\n",
    "\n",
    "# Homework 4: Clustering\n",
    "\n",
    "### 100 points [7% of your final grade]\n",
    "### Due: October 21 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main objectives of this homework: (i) implement k-means from scratch, (ii)  apply k-means on a toy dataset (the iris data), and (iii) explore the clustering of text data.\n",
    "\n",
    "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw4.ipynb`. For example, if your UIN is `123456789`, then your homework submission would be something like `123456789_hw4.ipynb`. Submit this notebook via Canvas (looking for homework 4 under the assignment section; detailed instructions can be found [here](https://community.canvaslms.com/t5/Student-Guide/How-do-I-upload-a-file-as-an-assignment-submission-in-Canvas/ta-p/274)). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after October 24 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration declarations:\n",
    "\n",
    "You should add in all of your collaboration declarations here. \n",
    "\n",
    "* Collaboration Declaration 1\n",
    "* Collaboration Declaration 2\n",
    "* ...\n",
    "\n",
    "*Recall our homework collaboration policy: Your homework is yours alone and you are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. If you do have a chat with another student about a homework problem, you must inform us by writing a note on your homework submission (e.g., Bob pointed me to the relevant section for problem 3). The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 1: Implement k-means\n",
    "  \n",
    "For the first part of this assignment, you are going to implement k-means from (almost) scratch, which means you can only use [The Python Standard Library](https://docs.python.org/3/library/). \n",
    "We have predefined some helper functions for the ease of implementation and grading. However, you should always keep the main function's structure in mind while implementing those helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the scope of The Python Standard Library, import all packages you need for the implementation of k-means\n",
    "import random\n",
    "from math import *\n",
    "from collections import namedtuple\n",
    "from copy import copy\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function 1:** Initialize Centroids  \n",
    "To start with, you need to pick k initial centroids from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeCentroids(datapoints, k):\n",
    "    centroids = random.sample(datapoints,k)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: you may fake some simple data to test your implementation of initializeCentroids here\n",
    "### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function 2:** Termination Condition  \n",
    "Next, you should determine the termination condition for the loop. This function should return True or False if k-means is done. K-means terminates either because it has run a maximum number of iterations OR the centroids stop changing (your implementation should handle both cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination(oldCentroids, centroids, iterations, max_iterations):\n",
    "    count = 0\n",
    "    if(max_iterations == iterations):\n",
    "        return True\n",
    "    if((centroids == None) or (oldCentroids == None)):\n",
    "        return False\n",
    "    if(len(centroids) == len(oldCentroids)):\n",
    "        for i in range(len(oldCentroids)):\n",
    "            if(centroids[i] == oldCentroids[i]):\n",
    "                count += 1\n",
    "            if(count == len(oldCentroids)):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: you may fake some simple data to test your implementation of termination here\n",
    "### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function 3:** Assign Cluster  \n",
    "Now, you can place each datapoint in the cluster whose current centroid is nearest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignCluster(datapoints, centroids):\n",
    "    clusters = [[],[],[]]\n",
    "    dist = []\n",
    "    p1 = 0\n",
    "    oncce = 0\n",
    "    dim = len(datapoints[0])\n",
    "    for i in datapoints:\n",
    "        for j in centroids:\n",
    "            for d in range(dim):\n",
    "                if(isinstance(j[d],list)):\n",
    "                    once = j[d][0]\n",
    "                else:\n",
    "                    once = j[d]\n",
    "                p1 += (i[d]-once)**2\n",
    "            p1 = sqrt(p1)\n",
    "            dist.append(p1)\n",
    "        if((dist[0] < dist[1]) and (dist[0] < dist[2])):\n",
    "            minimum = dist[0]\n",
    "        elif((dist[1] < dist[2]) and (dist[1] < dist[0])):\n",
    "            minimum = dist[1]\n",
    "        else:\n",
    "            minimum = dist[2]\n",
    "        clusters[dist.index(minimum)].append(i)\n",
    "        dist=[]\n",
    "        p1 = 0\n",
    "    return clusters #Clusters a list of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: you may fake some simple data to test your implementation of assignCluster here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function 4:** Calculate Centroids  \n",
    "Finally, the last helper function that calculates the new centroid of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCentroids(clusters):\n",
    "    centroids = []\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for a in clusters:\n",
    "        size = len(a)\n",
    "        divis = len(a)\n",
    "        for b in range(0,4):\n",
    "            sum1 = 0\n",
    "            for c in a:\n",
    "                sum1 += c[b]\n",
    "            if(b == 0):\n",
    "                centroids.append([sum1/(divis*1.0)])\n",
    "            else:\n",
    "                centroids[count].append([sum1/(divis*1.0)])\n",
    "        count +=1\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: you may fake some simple data to test your implementation of calculateCentroids here\n",
    "### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Function**  \n",
    "It's time to wrap everything up! This function takes datapoints, k, and max_iteration as input; and it returns **a list of length k in which each element is a cluster of datapoints** and **a list of k centroids**. Also, you may want to **preserve the corresponding order of these two lists for the ease of implementation in part 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(datapoints, k, max_iterations):\n",
    "    # initialization\n",
    "    datapoints = datapoints.values.tolist()\n",
    "    iterations=0\n",
    "    dim = len(datapoints[0])\n",
    "    oldCentroids = None\n",
    "    centroids = initializeCentroids(datapoints, k)\n",
    "    # Run the main k-means algorithm\n",
    "    while not termination(oldCentroids, centroids, iterations, max_iterations):\n",
    "        oldCentroids = centroids\n",
    "        # Update recordings\n",
    "        iterations += 1\n",
    "        \n",
    "        # Assign clusters based on centroids\n",
    "        clusters = assignCluster(datapoints, centroids)\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = calculateCentroids(clusters)\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 2: Clustering Irises\n",
    "For part 2 of this assignment, let's apply the k-means clustering function for plant taxonomy. Specifically, we will try to cluster some irises based on their sepal's and petal's length and width.  \n",
    "\n",
    "**For this part, you will need to:**\n",
    "1. Apply the k-means clustering function you just implemented on the iris data with k=1,2,3,...,9,10;\n",
    "2. Calculate the average distance to centroid for each k;\n",
    "3. Plot the k vs. average distance to centroid graph to pick the best value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width   petal_length   petal_width\n",
      "0           5.1          3.5            1.4           0.2\n",
      "1           4.9          3.0            1.4           0.2\n",
      "2           4.7          3.2            1.3           0.2\n",
      "3           4.6          3.1            1.5           0.2\n",
      "4           5.0          3.6            1.4           0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datapoints = pd.read_csv('Iris.tsv',sep='\\t')\n",
    "print(datapoints.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[7.0, 3.2, 4.7, 1.4],\n",
       "   [6.4, 3.2, 4.5, 1.5],\n",
       "   [6.9, 3.1, 4.9, 1.5],\n",
       "   [5.5, 2.3, 4.0, 1.3],\n",
       "   [6.5, 2.8, 4.6, 1.5],\n",
       "   [5.7, 2.8, 4.5, 1.3],\n",
       "   [6.3, 3.3, 4.7, 1.6],\n",
       "   [6.6, 2.9, 4.6, 1.3],\n",
       "   [5.2, 2.7, 3.9, 1.4],\n",
       "   [5.9, 3.0, 4.2, 1.5],\n",
       "   [6.0, 2.2, 4.0, 1.0],\n",
       "   [6.1, 2.9, 4.7, 1.4],\n",
       "   [5.6, 2.9, 3.6, 1.3],\n",
       "   [6.7, 3.1, 4.4, 1.4],\n",
       "   [5.6, 3.0, 4.5, 1.5],\n",
       "   [5.8, 2.7, 4.1, 1.0],\n",
       "   [6.2, 2.2, 4.5, 1.5],\n",
       "   [5.6, 2.5, 3.9, 1.1],\n",
       "   [5.9, 3.2, 4.8, 1.8],\n",
       "   [6.1, 2.8, 4.0, 1.3],\n",
       "   [6.3, 2.5, 4.9, 1.5],\n",
       "   [6.1, 2.8, 4.7, 1.2],\n",
       "   [6.4, 2.9, 4.3, 1.3],\n",
       "   [6.6, 3.0, 4.4, 1.4],\n",
       "   [6.8, 2.8, 4.8, 1.4],\n",
       "   [6.7, 3.0, 5.0, 1.7],\n",
       "   [6.0, 2.9, 4.5, 1.5],\n",
       "   [5.5, 2.4, 3.8, 1.1],\n",
       "   [5.8, 2.7, 3.9, 1.2],\n",
       "   [6.0, 2.7, 5.1, 1.6],\n",
       "   [5.4, 3.0, 4.5, 1.5],\n",
       "   [6.0, 3.4, 4.5, 1.6],\n",
       "   [6.7, 3.1, 4.7, 1.5],\n",
       "   [6.3, 2.3, 4.4, 1.3],\n",
       "   [5.6, 3.0, 4.1, 1.3],\n",
       "   [5.5, 2.5, 4.0, 1.3],\n",
       "   [5.5, 2.6, 4.4, 1.2],\n",
       "   [6.1, 3.0, 4.6, 1.4],\n",
       "   [5.8, 2.6, 4.0, 1.2],\n",
       "   [5.6, 2.7, 4.2, 1.3],\n",
       "   [5.7, 3.0, 4.2, 1.2],\n",
       "   [5.7, 2.9, 4.2, 1.3],\n",
       "   [6.2, 2.9, 4.3, 1.3],\n",
       "   [5.7, 2.8, 4.1, 1.3],\n",
       "   [6.3, 3.3, 6.0, 2.5],\n",
       "   [5.8, 2.7, 5.1, 1.9],\n",
       "   [7.1, 3.0, 5.9, 2.1],\n",
       "   [6.3, 2.9, 5.6, 1.8],\n",
       "   [6.5, 3.0, 5.8, 2.2],\n",
       "   [7.6, 3.0, 6.6, 2.1],\n",
       "   [4.9, 2.5, 4.5, 1.7],\n",
       "   [7.3, 2.9, 6.3, 1.8],\n",
       "   [6.7, 2.5, 5.8, 1.8],\n",
       "   [7.2, 3.6, 6.1, 2.5],\n",
       "   [6.5, 3.2, 5.1, 2.0],\n",
       "   [6.4, 2.7, 5.3, 1.9],\n",
       "   [6.8, 3.0, 5.5, 2.1],\n",
       "   [5.7, 2.5, 5.0, 2.0],\n",
       "   [5.8, 2.8, 5.1, 2.4],\n",
       "   [6.4, 3.2, 5.3, 2.3],\n",
       "   [6.5, 3.0, 5.5, 1.8],\n",
       "   [7.7, 3.8, 6.7, 2.2],\n",
       "   [7.7, 2.6, 6.9, 2.3],\n",
       "   [6.0, 2.2, 5.0, 1.5],\n",
       "   [6.9, 3.2, 5.7, 2.3],\n",
       "   [5.6, 2.8, 4.9, 2.0],\n",
       "   [7.7, 2.8, 6.7, 2.0],\n",
       "   [6.3, 2.7, 4.9, 1.8],\n",
       "   [6.7, 3.3, 5.7, 2.1],\n",
       "   [7.2, 3.2, 6.0, 1.8],\n",
       "   [6.2, 2.8, 4.8, 1.8],\n",
       "   [6.1, 3.0, 4.9, 1.8],\n",
       "   [6.4, 2.8, 5.6, 2.1],\n",
       "   [7.2, 3.0, 5.8, 1.6],\n",
       "   [7.4, 2.8, 6.1, 1.9],\n",
       "   [7.9, 3.8, 6.4, 2.0],\n",
       "   [6.4, 2.8, 5.6, 2.2],\n",
       "   [6.3, 2.8, 5.1, 1.5],\n",
       "   [6.1, 2.6, 5.6, 1.4],\n",
       "   [7.7, 3.0, 6.1, 2.3],\n",
       "   [6.3, 3.4, 5.6, 2.4],\n",
       "   [6.4, 3.1, 5.5, 1.8],\n",
       "   [6.0, 3.0, 4.8, 1.8],\n",
       "   [6.9, 3.1, 5.4, 2.1],\n",
       "   [6.7, 3.1, 5.6, 2.4],\n",
       "   [6.9, 3.1, 5.1, 2.3],\n",
       "   [5.8, 2.7, 5.1, 1.9],\n",
       "   [6.8, 3.2, 5.9, 2.3],\n",
       "   [6.7, 3.3, 5.7, 2.5],\n",
       "   [6.7, 3.0, 5.2, 2.3],\n",
       "   [6.3, 2.5, 5.0, 1.9],\n",
       "   [6.5, 3.0, 5.2, 2.0],\n",
       "   [6.2, 3.4, 5.4, 2.3],\n",
       "   [5.9, 3.0, 5.1, 1.8]],\n",
       "  [[5.1, 3.5, 1.4, 0.2],\n",
       "   [4.9, 3.0, 1.4, 0.2],\n",
       "   [4.7, 3.2, 1.3, 0.2],\n",
       "   [4.6, 3.1, 1.5, 0.2],\n",
       "   [5.0, 3.6, 1.4, 0.2],\n",
       "   [5.4, 3.9, 1.7, 0.4],\n",
       "   [4.6, 3.4, 1.4, 0.3],\n",
       "   [5.0, 3.4, 1.5, 0.2],\n",
       "   [4.4, 2.9, 1.4, 0.2],\n",
       "   [4.9, 3.1, 1.5, 0.1],\n",
       "   [5.4, 3.7, 1.5, 0.2],\n",
       "   [4.8, 3.4, 1.6, 0.2],\n",
       "   [4.8, 3.0, 1.4, 0.1],\n",
       "   [4.3, 3.0, 1.1, 0.1],\n",
       "   [5.8, 4.0, 1.2, 0.2],\n",
       "   [5.7, 4.4, 1.5, 0.4],\n",
       "   [5.4, 3.9, 1.3, 0.4],\n",
       "   [5.1, 3.5, 1.4, 0.3],\n",
       "   [5.7, 3.8, 1.7, 0.3],\n",
       "   [5.1, 3.8, 1.5, 0.3],\n",
       "   [5.4, 3.4, 1.7, 0.2],\n",
       "   [5.1, 3.7, 1.5, 0.4],\n",
       "   [4.6, 3.6, 1.0, 0.2],\n",
       "   [5.1, 3.3, 1.7, 0.5],\n",
       "   [4.8, 3.4, 1.9, 0.2],\n",
       "   [5.0, 3.0, 1.6, 0.2],\n",
       "   [5.0, 3.4, 1.6, 0.4],\n",
       "   [5.2, 3.5, 1.5, 0.2],\n",
       "   [5.2, 3.4, 1.4, 0.2],\n",
       "   [4.7, 3.2, 1.6, 0.2],\n",
       "   [4.8, 3.1, 1.6, 0.2],\n",
       "   [5.4, 3.4, 1.5, 0.4],\n",
       "   [5.2, 4.1, 1.5, 0.1],\n",
       "   [5.5, 4.2, 1.4, 0.2],\n",
       "   [4.9, 3.1, 1.5, 0.1],\n",
       "   [5.0, 3.2, 1.2, 0.2],\n",
       "   [5.5, 3.5, 1.3, 0.2],\n",
       "   [4.9, 3.1, 1.5, 0.1],\n",
       "   [4.4, 3.0, 1.3, 0.2],\n",
       "   [5.1, 3.4, 1.5, 0.2],\n",
       "   [5.0, 3.5, 1.3, 0.3],\n",
       "   [4.5, 2.3, 1.3, 0.3],\n",
       "   [4.4, 3.2, 1.3, 0.2],\n",
       "   [5.0, 3.5, 1.6, 0.6],\n",
       "   [5.1, 3.8, 1.9, 0.4],\n",
       "   [4.8, 3.0, 1.4, 0.3],\n",
       "   [5.1, 3.8, 1.6, 0.2],\n",
       "   [4.6, 3.2, 1.4, 0.2],\n",
       "   [5.3, 3.7, 1.5, 0.2],\n",
       "   [5.0, 3.3, 1.4, 0.2]],\n",
       "  [[4.9, 2.4, 3.3, 1.0],\n",
       "   [5.0, 2.0, 3.5, 1.0],\n",
       "   [5.7, 2.6, 3.5, 1.0],\n",
       "   [5.5, 2.4, 3.7, 1.0],\n",
       "   [5.0, 2.3, 3.3, 1.0],\n",
       "   [5.1, 2.5, 3.0, 1.1]]],\n",
       " [[6.329787234042552,\n",
       "   [2.904255319148936],\n",
       "   [5.003191489361702],\n",
       "   [1.7180851063829794]],\n",
       "  [5.005999999999999, [3.4180000000000006], [1.464], [0.2439999999999999]],\n",
       "  [5.2, [2.3666666666666667], [3.3833333333333333], [1.0166666666666666]]])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "kmeans(datapoints,3,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the best value of k according to your graph?**  \n",
    "``TODO``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Clustering Text Data\n",
    "For the 3rd part of this assignment, you will explore the clustering of text data. The good news: for this part, you can use scikit-learn!\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "      product_type                                     review_content\n",
      "0  Washing_Machine  Electrolux's Editors' Choice award-winning $1,...\n",
      "1  Washing_Machine  GE's $699 GTW685BSLWS top-load washing machine...\n",
      "2  Washing_Machine  Samsung's $1,899 WV9900 FlexWash is actually t...\n",
      "3  Washing_Machine  The $650 GE GTW485ASJWS might be one of the si...\n",
      "4  Washing_Machine  This $1,200 top-load washing machine seemingly...\n",
      "\n",
      "Shape:\n",
      "(7758, 2)\n",
      "\n",
      "Value Counts:\n",
      "Phone              1212\n",
      "Laptop             1069\n",
      "Camera              834\n",
      "TV                  794\n",
      "Headphone           653\n",
      "Speaker             551\n",
      "Printer             359\n",
      "Networking          355\n",
      "Tablet              282\n",
      "Monitor             258\n",
      "Blu-Ray_Player      229\n",
      "Desktop             216\n",
      "Video_Camera        206\n",
      "AV_Receiver         149\n",
      "Small_Appliance     114\n",
      "Wearable            107\n",
      "Media_Streamers      91\n",
      "Oven                 53\n",
      "Refrigerator         49\n",
      "Coffee_Maker         49\n",
      "Vacuum_Cleaner       45\n",
      "Washing_Machine      38\n",
      "Dryer                23\n",
      "Dishwasher           16\n",
      "Microwave             6\n",
      "Name: product_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('product_reviews.tsv',sep='\\t')\n",
    "print('Preview:')\n",
    "print(df.head())\n",
    "print('\\nShape:')\n",
    "print(df.shape)\n",
    "print('\\nValue Counts:')\n",
    "print(df['product_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 7758 product review articles of 25 different types of products. **The goal for this part is to derive a fixed smaller amount of supergroups (say 5~10 clusters; pick your favorite number!) with different clustering algorithms based on the content of reviews of 25 different types of products.**  \n",
    "Before feeding the clustering functions, you need to transform the data a little bit so that the clustering functions can digest the review articles.  \n",
    "\n",
    "First, you should represent each review as a vector, where element i of the vector corresponds to the count of word i in that product review. This is the classic [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Besides that, you also need to use lower case text, do tokenization, remove punctuations, remove [stop words](https://en.wikipedia.org/wiki/Stop_word), use [n-gram](https://en.wikipedia.org/wiki/N-gram) tokens, etc. for better results.   \n",
    "Now, let's implement all those things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't actually add any code here ... keep reading ... ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Wait a minute!</span> sklearn can do it for your in one line : )**  \n",
    "Take a look at [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and apply it to our corpus below. Alternatively, you can opt to implement all those things in the block above with **NO** extra credits :P  \n",
    "**CountVectorizer has a lot of parameters; set them wisely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['battery', 'better', 'camera', 'display', 'features', 'good', 'inch', 'just', 'like', 'll', 'mode', 'phone', 'quality', 'samsung', 'screen', 'sound', 'time', 'tv', 'use', 'video']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = df.loc[:, 'review_content'].to_list()\n",
    "Vectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2),strip_accents='unicode',max_features=20)\n",
    "M = Vectorizer.fit_transform(corpus)\n",
    "print(Vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than simply consider word counts as the representation of reviews (as we do in CountVectorizer), we can also consider alternative weightings of the words in our vector representation of each review. One of the classic approaches is [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf), which weighs a term in a document highly if it is popular within the document (term frequency) and also rare across all documents (inverse document frequency). Take a look at [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and apply it to our corpus below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['battery', 'better', 'camera', 'display', 'features', 'good', 'inch', 'just', 'like', 'll', 'mode', 'phone', 'quality', 'samsung', 'screen', 'sound', 'time', 'tv', 'use', 'video']\n",
      "[1.76072335 1.44664159 1.99384481 1.71017495 1.41745838 1.29756882\n",
      " 1.51413909 1.28515059 1.17091629 1.41296838 1.79666208 2.02748225\n",
      " 1.43263856 2.29885347 1.47878027 1.88789255 1.45635827 2.31456434\n",
      " 1.26950401 1.61615273]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "TfidVecto = TfidfVectorizer(strip_accents='unicode',stop_words='english',max_features=20)\n",
    "X = TfidVecto.fit_transform(corpus)\n",
    "print(TfidVecto.get_feature_names())\n",
    "M1 = TfidfTransformer(smooth_idf=True)\n",
    "M1.fit(X)\n",
    "print(M1.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to do clustering! In the following sections, apply k-means and another clustering algorithm of your choice (within the scope of [sklearn.cluster](https://scikit-learn.org/stable/modules/clustering.html)) to the product review corpus. Try both count vectors and tf-idf vectors, and record:\n",
    "- Cluster Composition by Count: for each cluster, count the amount of reviews by product_type;\n",
    "- Cluster Composition by Percentage: for each product_type, calculate its reviews' distribution (in percentage) across clusters.\n",
    "\n",
    "Recall that you need to pick a number between 5 and 10 as your target amount of supergroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "K-Means clustering by count\n",
      "Product Count for each cluster product_type     Cluster\n",
      "AV_Receiver      0          55\n",
      "                 1          73\n",
      "                 2           1\n",
      "                 6          20\n",
      "Blu-Ray_Player   0           3\n",
      "                            ..\n",
      "Video_Camera     6          35\n",
      "Washing_Machine  1          38\n",
      "Wearable         1          72\n",
      "                 4          22\n",
      "                 6          13\n",
      "Length: 84, dtype: int64\n",
      "Total values in each cluster 1    4470\n",
      "6    1006\n",
      "3     696\n",
      "4     666\n",
      "0     412\n",
      "2     406\n",
      "5     102\n",
      "Name: kmean, dtype: int64\n",
      "\n",
      "\n",
      "K-Means clustering by tf-idf\n",
      "Product Count for each cluster product_type  Cluster\n",
      "AV_Receiver   0          109\n",
      "              1            1\n",
      "              3            6\n",
      "              4           26\n",
      "              5            6\n",
      "                        ... \n",
      "Wearable      1           15\n",
      "              3            1\n",
      "              4           47\n",
      "              5           13\n",
      "              6           31\n",
      "Length: 117, dtype: int64\n",
      "Total values in each cluster 1    1594\n",
      "4    1434\n",
      "6    1183\n",
      "0    1071\n",
      "2    1063\n",
      "3     847\n",
      "5     566\n",
      "Name: kmean, dtype: int64\n",
      "\n",
      "\n",
      "Spectral Clustering by count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\AnacondaDS\\envs\\cs320\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "#### K-Means - Count\n",
    "print(\"\\n\\nK-Means clustering by count\")\n",
    "kmeans = cluster.KMeans(n_clusters=7)\n",
    "kmeans.fit_predict(M)\n",
    "labels = kmeans.labels_\n",
    "TypeCount = pd.DataFrame(labels,index=df['product_type'],columns=['Cluster'])\n",
    "Product_count = TypeCount.groupby(['product_type','Cluster']).size()\n",
    "print(\"Product Count for each cluster\",Product_count)\n",
    "TypeCount['kmean'] = kmeans.labels_\n",
    "print(\"Total values in each cluster\",TypeCount['kmean'].value_counts())\n",
    "#### K-Means - TF-IDF\n",
    "print(\"\\n\\nK-Means clustering by tf-idf\")\n",
    "kmeans.fit_predict(X)\n",
    "labels = kmeans.labels_\n",
    "TypeCount_TF = pd.DataFrame(labels,index=df['product_type'],columns=['Cluster'])\n",
    "Product_count = TypeCount_TF.groupby(['product_type','Cluster']).size()\n",
    "print(\"Product Count for each cluster\",Product_count)\n",
    "TypeCount_TF['kmean'] = kmeans.labels_\n",
    "print(\"Total values in each cluster\",TypeCount_TF['kmean'].value_counts())\n",
    "##### Spectral by Count\n",
    "print(\"\\n\\nSpectral Clustering by count\")\n",
    "SpClust = cluster.SpectralClustering(n_clusters=7)\n",
    "SpClust.fit_predict(M)\n",
    "labels = SpClust.labels_\n",
    "TypeCount_SP_Cnt = pd.DataFrame(labels,index=df['product_type'],columns=['Cluster'])\n",
    "Product_count_SP_Cnt = TypeCount_SP_Cnt.groupby(['product_type','Cluster']).size()\n",
    "print(\"Product Count for each cluster\", Product_count_SP_Cnt)\n",
    "TypeCount_SP_Cnt['Spectral'] = SpClust.labels_\n",
    "print(\"Total values in each cluster\",TypeCount_SP_Cnt['Spectral'].value_counts())\n",
    "##### Spectral by TF-IDF\n",
    "print(\"\\n\\nSpectral Clustering by tf-idf\")\n",
    "SpClust = cluster.SpectralClustering(n_clusters=7)\n",
    "SpClust.fit_predict(X)\n",
    "labels = SpClust.labels_\n",
    "TypeCount_SP = pd.DataFrame(labels,index=df['product_type'],columns=['Cluster'])\n",
    "Product_count_SP = TypeCount_SP.groupby(['product_type','Cluster']).size()\n",
    "print(\"Product Count for each cluster\", Product_count_SP)\n",
    "TypeCount_SP['Spectral'] = SpClust.labels_\n",
    "print(\"Total values in each cluster\",TypeCount_SP['Spectral'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the following table or use plots to illustrate your experiment results:  \n",
    "\n",
    "|  Model  | Vector | Cluster Composition by Count | Cluster Composition by Percentage |\n",
    "|:-------:|:------:|:----------------------------:|:---------------------------------:|\n",
    "| k-means |  count |            Max=1023 at Cluster 5            |               `TODO`              |\n",
    "| k-means | tf-idf |            Max=1417 at Cluster 4            |               `TODO`              |\n",
    "|  Ward hierarchical clustering |  count |      Max=4480 at Cluster 1         |       `TODO`              |\n",
    "|  Ward hierarchical clustering | tf-idf |            Max=1841 at Cluster 1           |               `TODO`              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze your experiment results and add a brief discussion here on:\n",
    "- which setting yields better clusters (supergroups)?\n",
    "- what're your justifications for your answer of the above question?\n",
    "- any other observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
